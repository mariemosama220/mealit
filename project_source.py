# -*- coding: utf-8 -*-
"""loaded final sent version.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1dS9qKEfmWlJAWy6TRJiKBGZF3xUwkavG

# import
"""

import time
start_time = time.time()

"""
we make recommendation based on the similarity of the items for food recipes based on the ingredients and recipe specifications using the cosine similarity method
and train word2vec model from scratch using gensim library on my dataset and then use the trained model to find the similarity between the ingredients of the recipes.
"""

# Importing the libraries
# !pip install textblob
# !pip install POT
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import re
import nltk
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
from nltk.tokenize import word_tokenize
#from sklearn.feature_extraction.text import CountVectorizer
#from sklearn.metrics.pairwise import cosine_similarity
from gensim.models import Word2Vec
#from sklearn.decomposition import PCA
#from sklearn.manifold import TSNE
import warnings
import gdown

import os

# pd.set_option('display.max_colwidth', None)
nltk.download('stopwords')
nltk.download('punkt')
warnings.filterwarnings('ignore')
nltk.download('wordnet')
# show all columns and rows
pd.set_option('display.max_columns', None)
pd.set_option('display.max_rows', None)

# # Importing the dataset
# df = pd.read_csv('https://github.com/eslamahmedamit/food-recipes-recsys/blob/87cf871934c5eb6adb16759ab0d190fba921211a/recipes_data_graduation_project.csv?raw=true')

# # Data Preprocessing
# df.head()

# import pandas as pd
# # https://github.com/eslamahmedamit/food-recipes-recsys/blob/87cf871934c5eb6adb16759ab0d190fba921211a/recipes_data_graduation_project.csv
# df = pd.read_csv('https://github.com/eslamahmedamit/food-recipes-recsys/blob/87cf871934c5eb6adb16759ab0d190fba921211a/recipes_data_graduation_project.csv?raw=true')
# # df

# https://drive.google.com/file/d/18obaPDPu58OnzhnJgVYLzg1C-XR1ER-4/view
# https://drive.google.com/file/d/1vw6D0GTM8zAg7U_O0PAEwIEw16M-rt9X/view?usp=sharing

# if out_text_data.csv file in folder then no need to download

if not os.path.exists('out_text_data.csv'):
    print ("Downloading out_text_data.csv file")   
    output = 'out_text_data.csv'
    id = "18obaPDPu58OnzhnJgVYLzg1C-XR1ER-4"
    gdown.download(id=id, output=output, quiet=False)



if not os.path.exists('word2vec.model'):
    print ("Downloading word2vec.model file")   
    output = 'word2vec.model'
    id = "1vw6D0GTM8zAg7U_O0PAEwIEw16M-rt9X"
    gdown.download(id=id, output=output, quiet=False)
    # gdown --id "1vw6D0GTM8zAg7U_O0PAEwIEw16M-rt9X"


# !gdown --id "18obaPDPu58OnzhnJgVYLzg1C-XR1ER-4"
# !gdown --id "1vw6D0GTM8zAg7U_O0PAEwIEw16M-rt9X"

text_data = pd.read_csv('out_text_data.csv')
word2vec_model = Word2Vec.load("word2vec.model")

"""# Recommendation for Text Data

### preprocessing
"""

# print text cleaning steps
'''
 text cleaning steps
    1- remove html tags
    2- remove special characters
    3- remove punctuation
    4- remove stopwords
    5- lemmatization
    6- lowercase
    7- remove numbers
    8- remove extra spaces
    9- remove single characters
    10- remove short words

'''
from tqdm.notebook import tqdm


# 1- remove html tags
def remove_html_tags(text):
    clean = re.compile('<.*?>')
    return re.sub(clean, '', text)

# 2- remove special characters
def remove_special_characters(text):
    pattern = r'[^a-zA-z0-9\s]'
    text = re.sub(pattern, '', text)
    return text

# 3- remove punctuation
def remove_punctuation(text):
    pattern = r'[^\w\s]'
    text = re.sub(pattern, '', text)
    return text

# 4- remove stopwords
def remove_stopwords(text):
    stop_words = set(stopwords.words('english'))
    word_tokens = word_tokenize(text)
    filtered_sentence = [w for w in word_tokens if not w in stop_words]
    return ' '.join(filtered_sentence)

# 5- lemmatization
def lemmatization(text):
    lemmatizer = WordNetLemmatizer()
    word_tokens = word_tokenize(text)
    lemmatized_output = ' '.join([lemmatizer.lemmatize(w) for w in word_tokens])
    return lemmatized_output

# 6- lowercase
def lowercase(text):
    return text.lower()

# 7- remove numbers
def remove_numbers(text):
    pattern = r'[0-9]'
    text = re.sub(pattern, '', text)
    return text

# 8- remove extra spaces
def remove_extra_spaces(text):
    pattern = r'\s+'
    text = re.sub(pattern, ' ', text)
    return text

# 9- remove single characters
def remove_single_characters(text):
    pattern = r'\s+[a-zA-Z]\s+'
    text = re.sub(pattern, ' ', text)
    return text

# 10- remove short words
def remove_short_words(text):
    pattern = r'\W*\b\w{1,3}\b'
    text = re.sub(pattern, '', text)
    return text

# 11 - conver plural to singular using textblob library
def singular(text):
    from textblob import TextBlob
    text = TextBlob(text)
    text = text.words.singularize()
    return text

# 12 - remove most 3 common words ['tablespoon','teaspoon','cup']
def remove_common_words(text):
    common_words = ['tablespoon','teaspoon','cup']
    text = ' '.join([word for word in text.split() if word not in common_words])
    return text


# apply all text cleaning steps
def text_cleaning(text):
    text = remove_html_tags(text)
    text = remove_special_characters(text)
    text = remove_punctuation(text)
    # print(text)
    text = remove_stopwords(text)
    text = lemmatization(text)
    text = lowercase(text)
    text = remove_numbers(text)
    text = remove_extra_spaces(text)
    text = remove_single_characters(text)
    # text = remove_short_words(text)
    # add text to string
    text = remove_common_words(text)
    text = singular(text)
    text = ' '.join(text)

    try:
      pbar.update(1)
    except:
      pass
    return text

# apply text cleaning steps on text data
# for feature in text_features:
#     text_data[feature] = text_data[feature].apply(text_cleaning)


# create function to apply with progress bar
def apply_with_bar(org_data,column):
    global pbar

    pbar = tqdm(total=len(org_data))

    org_data[f'{column}_cleaned'] = org_data[column].apply(text_cleaning)
    print(f'Done cleaning {column}')
    pbar.close()

# # apply function to all text columns
# clean_features = ['summary']
# clean_features = ['name','summary','ingredients']
# clean_features = ['name', 'category', 'summary', 'ingredients','diet_type']
# for column in clean_features:
#     apply_with_bar(text_data,column)

"""## Recommend Function"""

# get similarity between two ingredients with different teqnique
# convert the ingredients to vector if the ingredients is in the vocabulary using get_vector_pre_trained function
# edit the function to split words
def get_vector_pre_trained(x,model):
    s_vec = w_vec = np.zeros(model.vector_size)

    for word in x.split():
        if word in model:
            w_vec = model[word]
            s_vec = s_vec + w_vec
    # if s_vec.all() == 0:
    #     s_vec = np.zeros(256)
    return s_vec

def edit_vectors(vector_str):
    # Use regular expressions to convert str to float
    vector_str = vector_str.replace("[[]]", '')
    vector_str = re.sub(r"\n", "", vector_str)
    vector_str = re.sub(r"\[", "", vector_str)
    vector_str = re.sub(r"\]", "", vector_str)

    vector_str = re.sub(r"  ", " ", vector_str)
    vector_str = vector_str.split(' ')

    vector_str = [float(element) for element in vector_str if element != '']

    return vector_str

# type(edit_vectors(text_data['summary_vector'][4]))#[:20]

	# ingredients_vector	name_vector	category_vector	summary_vector	diet_type_vector	all_text_vector

text_data['ingredients_vector'] = text_data['ingredients_vector'].apply(edit_vectors)
text_data['name_vector'] = text_data['name_vector'].apply(edit_vectors)
text_data['category_vector'] = text_data['category_vector'].apply(edit_vectors)
text_data['summary_vector'] = text_data['summary_vector'].apply(edit_vectors)
text_data['diet_type_vector'] = text_data['diet_type_vector'].apply(edit_vectors)
text_data['all_text_vector'] = text_data['all_text_vector'].apply(edit_vectors)

# now we need to build a recommender system based on the ingredients, name, category, summary, diet_type to predict the best recipe for the user with high similarity
# we will use cosine similarity to get the similarity between the user input and the recipes in the dataset
# we will use the ingredients, name, category, summary, diet_type as features to predict the best recipe for the user

def get_similar_recipe_full_model(test_df_input,no_of_recipes=5):
    clean_features = ['name', 'category', 'summary', 'ingredients','diet_type']
    for column in clean_features:
        # apply_with_bar(test_df_input,column)
        test_df_input[f'{column}_cleaned'] = test_df_input[column].apply(text_cleaning)

    # get the vector of the input
    ingredients_vector = get_vector_pre_trained(test_df_input['ingredients_cleaned'][0],word2vec_model.wv)
    name_vector = get_vector_pre_trained(test_df_input['name_cleaned'][0],word2vec_model.wv)
    category_vector = get_vector_pre_trained(test_df_input['category_cleaned'][0],word2vec_model.wv)
    summary_vector = get_vector_pre_trained(test_df_input['summary_cleaned'][0],word2vec_model.wv)
    diet_type_vector = get_vector_pre_trained(test_df_input['diet_type_cleaned'][0],word2vec_model.wv)

    # try to abbreviation the previous code
    # get the vector of the input


    # get the cosine similarity between the input and the recipes in the dataset
    text_data['ingredients_similarity'] = text_data['ingredients_vector'].apply(lambda x: word2vec_model.wv.cosine_similarities(ingredients_vector,[x])[0])
    text_data['name_similarity'] = text_data['name_vector'].apply(lambda x: word2vec_model.wv.cosine_similarities(name_vector,[x])[0])
    text_data['category_similarity'] = text_data['category_vector'].apply(lambda x: word2vec_model.wv.cosine_similarities(category_vector,[x])[0])
    text_data['summary_similarity'] = text_data['summary_vector'].apply(lambda x: word2vec_model.wv.cosine_similarities(summary_vector,[x])[0])
    text_data['diet_type_similarity'] = text_data['diet_type_vector'].apply(lambda x: word2vec_model.wv.cosine_similarities(diet_type_vector,[x])[0])
    # get the mean of the similarity between the input and the recipes in the dataset
    text_data['mean_similarity'] = (text_data['ingredients_similarity'] + text_data['name_similarity'] + text_data['category_similarity'] + text_data['summary_similarity'] + text_data['diet_type_similarity'])/5
    # get the top 5 recipes with high similarity

    top_recipes = text_data.sort_values(by='mean_similarity',ascending=False).head(no_of_recipes)
    return top_recipes

# test the function
test_name = 'Macaroni and Cheese'
test_category = 'main-dish'
test_summary = 'This is a recipe for macaroni and cheese.'
test_ingredients = 'macaroni, cheese, butter, milk'
test_diet_type = 'Vegetarian'

# add to dataframe

test_df = pd.DataFrame({'name':[test_name],'category':[test_category],'summary':[test_summary],'ingredients':[test_ingredients],'diet_type':[test_diet_type]})

# get_similar_recipe_full_model(test_df)

# make a function to get the top 5 recipes with high similarity but similar with free text input and with all text column
def get_similar_recipe_free_text(test_text_input,no_of_recipes=5):
    # get the vector of the input
    text_vector = get_vector_pre_trained(test_text_input,word2vec_model.wv)
    # get the cosine similarity between the input and the recipes in the dataset
    text_data['text_similarity'] = text_data['all_text_vector'].apply(lambda x: word2vec_model.wv.cosine_similarities(text_vector,[x])[0])
    # get the top 5 recipes with high similarity
    top_recipes = text_data.sort_values(by='text_similarity',ascending=False).head(no_of_recipes)
    return top_recipes


# test the function
test_text = 'This is a recipe for macaroni and cheese.'
# get_similar_recipe_free_text(test_text)

# Make final function merge the two functions together based on mode parameter to get free text mode or not full model
# make the function input as a dictionary
def get_similar_recipe(user_input):
    # print(user_input)
    # print("test_input type is:",type(user_input))
    # print("*"*50)
    if user_input['mode']== 'full_model':
        # convert to dataframe from dictionary to dataframe using keys as columns

        # print(user_input)
        # print("test_input type is:",type(user_input))
        user_input_df = pd.DataFrame(user_input,index=[0])
        print(user_input_df)
        # user_input_df = pd.DataFrame(user_input['name'],user_input['category'],user_input['summary'],user_input['ingredients'],user_input['diet_type'])

        return get_similar_recipe_full_model(user_input_df,user_input['no_of_recipes'])
    elif user_input['mode'] == 'free_text':
        return get_similar_recipe_free_text(user_input['text'],user_input['no_of_recipes'])
    else:
        return 'Please enter the correct mode'


# test the function
test_input = {'name':'Macaroni and Cheese','category':'main-dish','summary':'This is a recipe for macaroni and cheese.',
              'ingredients':'macaroni, cheese, butter, milk','diet_type':'Vegetarian',
              'mode':'full_model','no_of_recipes':5}


get_similar_recipe(test_input)

# test the function with free text mode
test_input = {'text':'This is a recipe for macaroni and cheese.','mode':'free_text','no_of_recipes':5}
get_similar_recipe(test_input)

print("--- %s seconds ---" % (time.time() - start_time))

# !pip install flask
# !pip install pyngrok

# import sys
# from pyngrok import ngrok
# from flask import Flask, request, jsonify
# import os

# app = Flask(__name__)
# # make app config "ENV" = "development" to make the app run in development mode
# app.config["ENV"] = "development"
# app.config["USE_NGROK"] = True


# @app.route('/get-recipe', methods=['POST'])
# def get_recipe():

#     json_data = request.get_json()  # Get the JSON data from the request
#     print(json_data)

#     # test_input = json_data['test_input']  # Extract the required input from the JSON
#     # print(test_input)
#     result = get_similar_recipe(json_data).to_json()  # Call your function with the input
#     print("result", jsonify(result)  )
#     return jsonify(result)  # Return the result as a JSON response

# if __name__ == '__main__':
#     app.run()

# if __name__ == '__main__':
#     app.run(host="localhost",port=13950, debug=False)


# create requirements.txt with used libraries in the project only not all libraries in the environment
# !pip freeze > requirements.txt